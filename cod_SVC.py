# -*- coding: utf-8 -*-
"""SVC_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1trlSNIS7NdU9QT5u0iNR8Tow5HAObeCK
"""

import os
import pandas as pd
import tensorflow as tf
from google.colab import drive

print(tf.test.is_gpu_available('GPU'))
drive.mount('/content/drive')

from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn import svm
import numpy as np
import pandas as pd

def readFile(path):

    file = open(path , 'r' , encoding = "utf-8")
    return file.readlines()

inputs_antrenare = readFile('/content/drive/MyDrive/Competitie/train_samples.txt') #READING DATA FROM COLAB
inputs_test = readFile('/content/drive/MyDrive/Competitie/test_samples.txt')
inputs_validare = readFile('/content/drive/MyDrive/Competitie/validation_samples.txt')
labels_antrenare = readFile('/content/drive/MyDrive/Competitie/train_labels.txt')
labels_validare = readFile('/content/drive/MyDrive/Competitie/validation_labels.txt')

print(inputs_test[0])

id_test = [int(x.split('\t')[0]) for x in inputs_test] #GETTING IDS FOR THE TEST DATA

def get_labels(file):

    labels = [int(label.split('\t')[1])-1 for label in file] #GETTING LABELS FROM DATASET
    return labels

labels_antrenare = get_labels(labels_antrenare)
labels_validare = get_labels(labels_validare)

print(labels_antrenare[0])

def removeIDS(dataSample):

    finalString = [row.split('\t')[1].replace('\n' , '') for row in dataSample] #REMOVEIDS
    return finalString

inputs_antrenare = removeIDS(inputs_antrenare)
inputs_test = removeIDS(inputs_test)
inputs_validare = removeIDS(inputs_validare)

from sklearn.utils import shuffle
inputs_antrenare, labels_antrenare = shuffle(inputs_antrenare, labels_antrenare, random_state=0) 
inputs_validare, labels_validare = shuffle(inputs_validare, labels_validare, random_state=0)

"""S = set()
for text in inputs_antrenare + inputs_validare + inputs_test: #STEMMING (UNSUCCESFULL)
  for wrd in text.split():
    S.add(wrd)

S = list(S)
S.sort(key = lambda x: len(x))
pairSet = []
for wrd in S:
  for wrd2 in S:
    if (len(wrd) == len(wrd2)+2 ) and (wrd.startswith(wrd2) or wrd.endswith(wrd2)) and len(wrd2)>=3:
      pairSet.append((wrd,wrd2))
    elif len(wrd) < len(wrd2)+2:
        break


for i in range(len(inputs_antrenare)):
  for (x,y) in pairSet:
    inputs_antrenare[i] = inputs_antrenare[i].replace(x,y)

for i in range(len(inputs_validare)):
  for (x,y) in pairSet:
    inputs_validare[i] = inputs_validare[i].replace(x,y)

for i in range(len(inputs_test)):
  for (x,y) in pairSet:
    inputs_test[i] = inputs_test[i].replace(x,y)
"""

print(inputs_antrenare[0])

inputs_antrenare = inputs_antrenare + inputs_validare[:3000] #CHANGING THE TRAIN/VALIDATION SPLIT
inputs_validare = inputs_validare[3000:]

labels_antrenare = labels_antrenare + labels_validare[:3000]
labels_validare = labels_validare[3000:]


#inputs_antrenare = inputs_antrenare[:100]
#labels_antrenare = labels_antrenare[:100]
inputs_antrenare,labels_antrenare = shuffle(inputs_antrenare, labels_antrenare, random_state = 0)

vectorizer = TfidfVectorizer(analyzer = 'char' , norm = 'l2', ngram_range  = (1, 3), lowercase = False) #NGRAM VECTORIZER USING TFIDF
vector = vectorizer.fit_transform(inputs_antrenare)

def transform(samples, vectorizer):
    return vectorizer.transform(samples)

inputs_antrenare = transform(inputs_antrenare, vectorizer) #VECTORIZING THE DATASET
inputs_test = transform(inputs_test, vectorizer)
inputs_validare = transform(inputs_validare, vectorizer)

print(inputs_antrenare.shape)

from keras.utils import np_utils
#labels_antrenare = np_utils.to_categorical(labels_antrenare)
#labels_validare = np_utils.to_categorical(labels_validare)
#print(labels_antrenare.shape)

from sklearn.svm import SVC
clasificator = SVC(kernel = 'linear',random_state = 0,C=3) #THE ACTUAL CLASSIFIER
clasificator.fit(inputs_antrenare, labels_antrenare)

predictions_test = clasificator.predict(inputs_test)
predictions_test = [x+1 for x in predictions_test] 
subm = pd.DataFrame({"id" : id_test, "label": predictions_test}) #OUTPUT THE SUBMISSION 
subm.to_csv("subm.csv", index = False)

clasificator.score(inputs_validare,labels_validare) #CHECK FOR VALIDATION ACCURACY

predictions_valid = clasificator.predict(inputs_validare) #CONFUSION MATRIX
aux = [x for x in labels_validare]

from sklearn import metrics
matriceConfuzie = metrics.confusion_matrix(aux, predictions_valid)


import seaborn 
seaborn.heatmap(matriceConfuzie,annot = True)