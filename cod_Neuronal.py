# -*- coding: utf-8 -*-
"""bestNGRAMNN (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jtiaXLnvNMt_OUD90okYEoeJbRVhCWrk
"""

import os
import pandas as pd
import tensorflow as tf
from google.colab import drive

print(tf.test.is_gpu_available('GPU'))
drive.mount('/content/drive')



from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
import numpy as np
import pandas as pd

def readFile(path):

    file = open(path , 'r' , encoding = "utf-8")
    return file.readlines()

inputs_antrenare = readFile('/content/drive/MyDrive/Competitie/train_samples.txt') #READING DATA FROM COLAB
inputs_test = readFile('/content/drive/MyDrive/Competitie/test_samples.txt')
inputs_validare = readFile('/content/drive/MyDrive/Competitie/validation_samples.txt')
labels_antrenare = readFile('/content/drive/MyDrive/Competitie/train_labels.txt')
labels_validare = readFile('/content/drive/MyDrive/Competitie/validation_labels.txt')

print(inputs_test[0])

id_test = [int(x.split('\t')[0]) for x in inputs_test] #GETTING THE TEST IDS

def getLabels(data):

    res = [int(x.split('\t')[1])-1 for x in data] #GETTING LABELS FROM DATASET
    return res

labels_antrenare = getLabels(labels_antrenare) 
labels_validare = getLabels(labels_validare)

print(labels_antrenare[0])

def removeIds(inputString):

    res = [row.split('\t')[1].replace("\n","") for row in inputString] #REMOVING IDS FROM DATASET
    return res

inputs_antrenare = removeIds(inputs_antrenare)
inputs_test = removeIds(inputs_test)
inputs_validare = removeIds(inputs_validare)

from sklearn.utils import shuffle
inputs_antrenare, labels_antrenare = shuffle(inputs_antrenare, labels_antrenare, random_state=42)
inputs_validare, labels_validare = shuffle(inputs_validare, labels_validare, random_state=42)

print(labels_antrenare)

"""#STOPWORDS FINDING ATTEMPT (UNSUCCESSFUL)
cuvLeg= []
D = [dict() for i in range(3)]
for (text,lab) in zip(inputs_antrenare,labels_antrenare):
  for wrd in text.split():
    if wrd not in D[lab].keys():
      D[lab][wrd]=1
    else:
      D[lab][wrd]+=1

for wrd in D[0].keys():
  if D[0][wrd]>1000 and D[1][wrd]>1000 and D[2][wrd]>1000:
    cuvLeg.append(wrd)

print(cuvLeg)
#print(D)

S = set()#STEMMING ATTEMPT (UNSUCCESSFUL)
for text in inputs_antrenare + inputs_validare + inputs_test:
  for wrd in text.split():
    S.add(wrd)

S = list(S)
S.sort(key = lambda x: len(x))
pairSet = []
for wrd in S:
  for wrd2 in S:
    if (len(wrd) == len(wrd2)+2 ) and (wrd.startswith(wrd2) or wrd.endswith(wrd2)) and len(wrd2)>=3:
      pairSet.append((wrd,wrd2))
    elif len(wrd) < len(wrd2)+2:
        break


for i in range(len(inputs_antrenare)):
  for (x,y) in pairSet:
    inputs_antrenare[i] = inputs_antrenare[i].replace(x,y)

for i in range(len(inputs_validare)):
  for (x,y) in pairSet:
    inputs_validare[i] = inputs_validare[i].replace(x,y)

for i in range(len(inputs_test)):
  for (x,y) in pairSet:
    inputs_test[i] = inputs_test[i].replace(x,y)
"
"""

#print(pairSet)

print(inputs_antrenare[0])

inputs_antrenare = inputs_antrenare + inputs_validare[:3000] #CHANGING THE SPLITS
inputs_validare = inputs_validare[3000:]

labels_antrenare = labels_antrenare + labels_validare[:3000]
labels_validare = labels_validare[3000:]

inputs_antrenare,labels_antrenare = shuffle(inputs_antrenare, labels_antrenare, random_state = 42)



vectorizer = TfidfVectorizer(analyzer = 'char' , ngram_range  = (1, 5), lowercase = False, norm = 'l1') #VECTORIZER
vector = vectorizer.fit_transform(inputs_antrenare)

def transformaDate(samples, vectorizer):
    return vectorizer.transform(samples)

inputs_antrenare = transformaDate(inputs_antrenare, vectorizer)
inputs_test = transformaDate(inputs_test, vectorizer)
inputs_validare = transformaDate(inputs_validare, vectorizer)

print(inputs_antrenare.shape)

def oneHot(x): #CONVERT TO ONE-HOT ENCODING
  arr = [0 for i in range(3)]
  arr[x] = 1
  return np.array(arr)



from keras.utils import np_utils
labels_antrenare = np.array([oneHot(x) for x in labels_antrenare])
labels_validare = np.array([oneHot(x) for x in labels_validare])
print(labels_antrenare.shape)





from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from keras.layers import Dense, Dropout


def initializeModel():

    # De incercat cu tanh!
    model = Sequential()
    model.add(Dense(1024, input_dim=447779, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='tanh'))
    model.add(Dense(3, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate = 1e-4/2), metrics=['accuracy'])
    model.summary()
  
    return model


clasificator = KerasClassifier(build_fn=initializeModel, epochs=15, batch_size=64)
inputs_antrenare.sort_indices()

inputs_validare.sort_indices()

clasificator.fit(inputs_antrenare, labels_antrenare, validation_data = (inputs_validare, labels_validare), verbose = 2)



inputs_test.sort_indices()
predictions_test = clasificator.predict(inputs_test)
predictions_test = [x+1 for x in predictions_test]
subm = pd.DataFrame({"id" : id_test, "label": predictions_test}) #SUBMISSION
subm.to_csv("submisie.csv", index = False)

predictions_valid = clasificator.predict(inputs_validare)

aux = [np.argmax(x) for x in labels_validare]

from sklearn import metrics
matriceConfuzie = metrics.confusion_matrix(aux, predictions_valid) #CONFUSION_MATRIX


import seaborn 
seaborn.heatmap(matriceConfuzie,annot=True)

#print(matrix)